---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.1'
      jupytext_version: 1.1.1
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

```{r include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.path="fig/")
```

```{r, echo=FALSE}
library(reticulate)
use_virtualenv('venv', require=TRUE)
```

# Reasoning about DAGs

1. Has everyone enrolled in Piazza?  
2. Office hours on Friday evening
3. Scribes (publish first)


```{python}
import torch
import pyro
pyro.set_rng_seed(101)
```

## Recap: Causal models as generative models

Our goal is to understand causal modeling within the context of generative machine learning. We just examined one generative machine learning framework called Bayesian networks (BNs) and how we can use BNs as causal models.

### Ladder of causality (slides)

* Associative:
  * Broad class of statistically learned models (discriminiative and generative)
  * Includes deep learning (unless you tweak it)
* Intervention
  * Why associative models can't do this -- changing the joint
  * Causal Bayes nets
  * Tradition PPL program
* Counterfactual
  * Causal Bayesian networks can't do this
  * Structural causal models -- can be implemented in a PPL

### Some definitions and notation

* Joint probability distribution: $P_{\mathbb{X}}$
* Density $P_{\mathbb{X}=x} = \pi(x_1, ..., x_d)$
* Bivariate $P_{Z, Y}$, marginal $P_{Z}$, conditional $P_{Z|Y}$
* Generative model $\mathbb{M}$ is a machine learning model that "entails" joint distribution, either explicitly or implicitly
* We denote the joint probability distribution "entailed" by a generative model as $P_{\mathbb{X}}^{\mathbb{M}}$
* Directed acyclic graph DAG $\mathbb(G) = (V, E)$, where E is a set of directed edges.
* Parents in the DAG: Parents of $X_j$ in the DAG $\mathbb(G)$ is denoted $\text{pa}_j^{\mathbb{G}}$
* A Bayesian network is a generative model that entails a joint distribution that factorizes over a DAG.
* A causal generative model is a generative model of a causal mechanism.
* A causal Bayesian networks is a causal generative model that is simply a Bayesian network where the direction of edges in the DAG represent causality.
* Probabilistic programming:  Writing generative models as program.  Usually done with a framework that provides a DSL and abstractions for inference
* "Causal program": Let's call this a probabilistic program that   As with a causal Bayesian network, you can write your program in a way that orders the steps of its execution according to cause and effect.

### Difference between Bayesian networks and probabilistic programming

* BNs have more constraint
  * Probabilistic relationships limited to conditional probability distributions (CPDs) factored according to a DAG.  
    * Frameworks typically limit you to a small set of parametric CPDs (e.g., Gaussian, multinomial).
    * bnlearn allows multinomial or ordinal variables for discrete, Gaussian for continuous.
* PPLs let you represent relations any way you like so long as you can represent them in code.
  * Nonparameterics
  * Strange distributions
  * Control flow and recursion
    * DAGs all variables are known in advance
    * "Open world models": control flow may create new variables (blackboard)
      ```
      X = Bernoulli(p)
      if X == 1:
          Y = Gaussian(0, 1)
      ```
      ```
      X = Poisson(Î»)
      Y = zeros(X)
      Y[0] = [Gaussian(0, 1)]
      for i in range(1, X):
          Y[i] = Gaussian(Y[i-1], 1))
      ```
* Inference ![Image](fig/inference.png)
  * Inference is easier in BNs given the constraints
    * PGM inference such as belief probagation, variable elimination
  * In PPLs inference is tougher
    * Require you to become something of an inference expert
    * That said, PPL developers provide inference abstractions so you don't have to work from scratch
    * PPLs use cutting-edge inference algorithms
      * Include tensor-based frameworks like Tensorflow and PyTorch, allow you to build on data science intuition.
      * Stochastic variation inference
      * Minibatching -- process groups of training examples simultaneously to take advantage of modern hardware like GPUs
  * Finally, it is easier to reason about the joint distribution if you have a DAG. 

## Reasoning with DAGs

###  Intuition

DAG is a graphical language for reasoning about joint probability distribution, and also reasoning about causality. DAGs have been used to represent causal and temporal relationships between variables.

### Reading DAGs as factorization

#### Recap on core concepts

**Conditional probability**:
  Given two nodes $X$ and $Y$, conditional probability can be represented as:
  $$P(X|Y) = P(X,Y)/P(Y)$$
  Now rearranging, the joint can be expressed as product of two small quantities.
  $$P(X,Y) = P(X|Y)P(Y)$$

**Conditional independence**:
  Given that we have observed $Z$, $X$ is conditionally independent of $Y$ in the probability distribution ${P_{\mathbb{A}}}$(denoted $X \perp_{P_{\mathbb{A}}} Y|Z$), if and only if the conditional joint probability can be written as product of conditional marginal probabilities i.e,
  $$P(X,Y| Z) = P(X|Z)P(Y|Z)$$
  Intuitively, this means that, once $Z$ is known $Y$ provides no additional information about $X$
  If $Z$ is conditionally independent of $X$ given $Y$ we can write:
  $$P(X,Y,Z) = P(X)P(Y|X)P(Z|Y)$$
  
  DAGs are useful for representing conditional independence relationship between variables. Lack of edges in DAG represent Conditional independence assumptions and hence, more such assumptions, lesser the edges in the graph. Conditional independence makes a DAG more compact.


### Core graphical concepts

#### Path 

A path in $\mathbb{G}$ is a sequence of (at least two) distinct vertices $i_1,...,i_m$, such that there is an edge between $i_k$ and $i_k+1$, for all $k=1,...,m-1$.

#### Pearl's d-separation

Consider three disjoint set of variables, X,Y and Z represented as nodes in a graph $\mathbb{G}$. To test whether X is independent of Y given Z, we need to test whether the nodes corresponding to variables $Z$ "blocks" all paths from X to Y. This is defined by d-separation.
Formally, a path $p$ is said to be d-separated by a set of nodes Z if and only if:
1. $p$ contains a chain $i\to m\to j$ or a fork $i\leftarrow  m\to j$, such that the middle node $m$ is in $Z$
2. p contains an inverted fork (or collide) $i\to m\leftarrow j$ such that the middle node
m is not in Z and such that no descendant of $m$ is in $Z$.
A set $Z$ is said to d-separated $X$ from $Y$ if and only if $Z$ **blocks** every path from a node in $X$
to a node in $Y$.

```{r fig.height=4, fig.width=4, echo=FALSE, fig.align='center', message=FALSE}
library(png)
library(grid)
img<- readPNG("fig/d-sep.png")
grid.raster(img)
```


In the above picture, $U$ is conditionally independent of $W$, given $V$ in the first three cases. Intuitively, in causal chains(1&2) and causal forks(3), $U$ and $W$ are marginally dependent, but, become independent of each other when $V$ is known. Conditioning on $V$ appears to block the flow of information along the path, so learning about $U$ will not effect the probability of $W$, once $V$ is observed. 

For example, in structure 1, consider $U$ to be Grandparent's genome, $V$ the parent's genome and $W$ is your genome information, and we know everything about the parent's genome($U$). Now, there is no new information about your genome that your grandparent's genome($U$) can provide, given the parent's genome($V$). A similar blockage of information in observed in the second case. In structure 3(common parent), $V$ is the parent's genome, if $U$ is the sibling's genome, $W$ is your genome. Now once the parent's genome is know, theses no new information the sibling's genome can provide that can explain your genome.

#### V-structures

V-structures, also know as colliders, or inverted forks, work in a different way. V-structures represents two causes having a common effect(structure 4 in the above figure). On observing the middle variable(effect $V$), the two extreme variables(causes $U$ and $W$) which were marginally independent, will now have an unblocked path between them, making them dependent, and this is true for any descendant for $V$
However, if the effect is not observed, the two variables causing it will remain independent.

This is a little unintuitive, so let us consider a simple example of a sprinkler.
```{r fig.height=4, fig.width =4, echo=FALSE, fig.align='center', message=FALSE}
library(png)
library(grid)
img<- readPNG("fig/sprinkler.png")
grid.raster(img)
```



Grass will be **wet** by two causes: when it rains(**Rain** = **yes**); when the sprinkler is on(**sprinkler** = **on**). Now lets say, we have observed that the grass is wet, and by some means(say, Google weather) we have the information that it has not rained(**Rain** = **no**). We now can conclude that the sprinkler was on(**sprinkler** = **on**). Generally, there is no correlation between rain and sprinkler, they are independent, but, when we observe the grass(the effect), the path is now **unblocked**, and this induces dependence among the causes(rain and sprinkler) 

This corresponds to the general pattern of causal relationships: observations on a common consequence of two independent causes tend to render those causes dependent, because information about one of the causes tends
to make the other more or less likely, given that the consequence has occurred.

There are two types of V-structures  
1. **immoral v-structure**: V-structure in which the parents are **not** linked by an arc.  
2. **moral v-structure**: V-structure in which the parents are linked by an arc.  

```{r fig.height=4, fig.width=5, echo=FALSE, fig.align='center', message=FALSE}
img<- readPNG("fig/v-structures.png")
grid.raster(img)
```



### What does conditional independence have to do with causality?

Consider 2 variables $X$ and $Y$, a correlation between them would mean that either $X$ causes $Y$ or $Y$ causes $X$. Correlation implies that one of the two variables is causal. Now, consider a graph $\mathbb{G}$ with three variables, $X$, $Y$ and $Z$ modeled as $X\to Y\to Z$, whose joint probability can be factorized as 
$$P(X)P(Y|X)P(Z|Y)$$  

This can lead to three equivalent factorization: 
$$P(X)P(Y|X)P(Y|X,Z)$$
$$P(Y)P(X|Y)P(Z|Y)$$ 
$$P(Z)P(Y|Z)P(X|Y)$$
And 3 equivalent DAGs. Now in such a case, correlation implies that one of these models, is a causal model. Using correlations, we may at least infer the existence of causal links from correlations, if not for a concrete causal graph. Conditional independence narrows down the causal negatives and reduces the problem to reasoning about the joint probability distribution to graph algorithms.

R's **bnlearn** library, includes a function **d-sep**, and Python's **pgmpy** library with modules local_independencies and get_independencies, can be used to test for d-separation, or to get d-separated nodes.

### Markov Property

#### Markov blanket  

The Markov blanket for a node in a graphical model contains all the variables that shield the node from the rest of the network. This means that the Markov blanket of a node is the only knowledge needed to predict the behavior of that node and its children.

In terms of joint probability, this would mean that every set of nodes in the network is conditionally independent of $A$, when conditioned on the Markov Blanket of $A$. Formally, 
$${P(A\mid \operatorname {MB} (A),B)=P(A\mid \operatorname {MB} (A))}$$
Where ${\operatorname {MB}(A)}$ is the set of nodes in the Markov Blanket of $A$


```{r fig.height=3, fig.width =3, echo=FALSE, fig.align='center', message=FALSE}
img<- readPNG("fig/markovBlanket.png")
grid.raster(img)
```
In Bayesian networks, the Markov blanket of node A includes its **parents**, **children** and the **other parents of all of its children**. In the above figure, the nodes in the blue circle is the Markov Blanket of node A. The Markov Blanket, d-separates A from all other nodes. If we were fitting a model, and we  want to include the Markov Blanket as predictors, any other predictor we add, is over fitting, since there is no new information that can be learnt using the other predictors.  

#### Markov properties

1. **Global** : A graph is globally Markov with respect to joint distribution if every d-sep inside the graph corresponds to conditional independence statement within the joint probability distribution ${P_\mathbb{X}}$.
  Formally,
$${U \perp_{\mathbb{G}} W|V \implies U \perp_{P_\mathbb{X}} W|V }$$  

2. **Local**: Every variable is conditionally independent of its non descendants given it parents. A well know example of local Markov property is a Markov chain.   

3. **Markov factorization**: If we can factorize a joint probability distribution by conditioning each node by its parents, then we satisfy Markov factorization property. This makes it a computational efficient way of evaluating the joint using logarithmic properties.
  $${P_\mathbb{X}=x}=\pi(x_1,...,x_d) = \prod_{j=1}^{d} \pi(x_j|pa_{j}^{\mathbb{G})}$$
  $$log(\pi(x_1,...,x_d)) = \sum_{j=1}^{d} \pi(x_j|pa_{j}^{\mathbb{G}})$$

These three properties are equivalent definitions, if one of them is true, the others are true.

#### Markov equivalence 

Conditional probability is given  as $P(A|B) = P(A,B)/P(B)$. Now, this definition means we can factorize any joint into a product of conditionals. Consider the joint factorization, $P(A, B, C) =  P(A)P(B|A)P(C|A, B)$.  

A DAG for such a factorization would have the edges {$A\to B, A\to C, B\to C$}. But, we can also factorize the joint, this way $P(A, B, C)=P(C)P(B|C)P(A|B,C)$, with edges {$C\to B, B\to A, C\to A$}. So, we have two different DAGs that are equivalent factorization of the joint probability. We call this equivalence a **Markov equivalence**. Generally, given a DAG, the set that includes that DAG and all the DAGs that are Markov equivalent to that DAG are called a **Markov equivalence class**.

When we have an equivalence class, we can look for some kind of isomorphism between two objects to test if they are equivalent, according to some definition of equivalence. The equivalence classes of DAGs have the same "skeleton", meaning the set of connections between nodes, but the direction of edges among some/all nodes would differ. **PDAG** is a compact representation of an equivalence class, and this serves well when its hard to enumerate the entire equivalence class of objects.  The PDAG has the same skeleton as all of the members of the equivalence class. The undirected edges in the PDAG correspond to edges that vary in direction among members of the class. A directed edge in the PDAG mean that  all members of the class have that edge oriented in that direction.

There are other graphical representations of joint probability distribution  
1. Undirected graph - All edges are bidirectional, and does not admit causal reasoning.    
2. Ancestral graphs - A type of mixed graph to provide a graphical representation for the result of marginalizing one or more vertices in a graphical model, this does not directly map to a generative model.

## Causality and DAGs
  * Assume no latent variables (very strong assumption)
  * Causation vs correlation -- only two options
  * A second look at PDAGs

  
